# -*- coding: utf-8 -*-
"""WikipediaAltChecker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KdjrkodgPmiIP4_hxw5H_k4Pqv9pYzzx
"""

# Author: CorraleH (https://meta.wikimedia.org/wiki/User:CorraleH)
# License: GNU General Public License v3.0 (https://www.gnu.org/licenses/gpl-3.0.en.html)
# https://github.com/CorraleH/Wikipedia-Alt-Checker

"""
This script uses the public MediaWiki API to inspect the pages contained
within a given Wikipedia category and reports which image links in those
pages provide alternative text (the ``alt`` parameter) and which do not.

The purpose of the program is to assist editors in locating pages that
contain images lacking alternative text, a key accessibility feature
described in the Portuguese Wikipedia guidelines.  According to those
guidelines, every non‑decorative image on Wikipedia should include a
concise alternative text specified via the ``alt`` parameter of the
image link.  Alternative text ensures that people using screen readers
or browsing with images disabled still receive the essential
information conveyed by the image【211833169631064†L150-L165】.

Usage:
    python wikipedia_alt_checker.py --category "Categoria:Animais" --lang pt

Options:
    --category, -c   Name of the category to inspect.  If the namespace
                     prefix (e.g. ``Categoria:`` or ``Category:``) is
                     omitted, it will be added automatically based on
                     the selected language.
    --lang, -l       Language code for the target Wikipedia (default
                     ``pt`` for Portuguese).  For English, use ``en``.
    --limit, -n      Optional maximum number of pages to inspect.  When
                     omitted, the script processes every page in the
                     category.  Use this to test the script on large
                     categories without exhausting API limits.

The script prints a summary for each page showing the total number of
image links, the number with an ``alt`` parameter and the number
without.  At the end, it prints aggregated totals across the entire
category.

Note:
    This code depends on the external ``requests`` library (for HTTP
    calls) and uses only the built‑in ``re`` and ``argparse`` modules
    otherwise.  No network requests are executed if the module is
    imported; all activity occurs within the ``main`` function.
"""

import argparse
import re
import sys
from typing import Iterable, List, Tuple

try:
    import requests
except ImportError as exc:  # pragma: no cover -- import error is obvious
    raise SystemExit(
        "The requests library is required to run this script. "
        "Install it with `pip install requests`"
    ) from exc


def normalize_category_name(category: str, lang: str) -> str:
    """Ensure the category name includes the appropriate namespace prefix.

    Wikipedia categories live in a dedicated namespace.  In English the
    prefix is ``Category:``, while in Portuguese it is ``Categoria:``.
    MediaWiki treats spaces and underscores as equivalent when
    referencing page titles【117812809752333†L151-L161】, but API calls
    commonly use underscores.  This helper also replaces spaces with
    underscores.

    Args:
        category: The user‑supplied category name, possibly without
            namespace prefix.
        lang:     The language code (e.g. ``en``, ``pt``).

    Returns:
        The normalized category title ready for API consumption.
    """
    category = category.strip()
    # Determine the namespace prefix based on language.
    if lang.lower() == "pt":
        prefix = "Categoria:"
    else:
        prefix = "Category:"
    # Add the prefix if it is missing (case‑insensitive comparison).
    if not category.lower().startswith(prefix.lower()):
        category = f"{prefix}{category}"
    # Convert spaces to underscores, since MediaWiki treats them the same
    # but API endpoints typically expect underscores【117812809752333†L151-L161】.
    category = category.replace(" ", "_")
    return category


def get_category_members(category: str, lang: str, limit: int | None = None) -> List[str]:
    """Retrieve the titles of all pages in a category.

    This function iterates through the MediaWiki ``categorymembers`` API
    endpoint, collecting titles until the entire category has been
    consumed or until ``limit`` pages have been gathered.  It filters
    results to pages in the main namespace (namespace 0) to avoid
    including other categories, templates or files.

    Args:
        category: Normalized category title (namespace included) with
            underscores instead of spaces.
        lang:     Language code for the target Wikipedia.
        limit:    Optional maximum number of page titles to return.

    Returns:
        A list of page titles (strings).
    """
    base_url = f"https://{lang}.wikipedia.org/w/api.php"
    session = requests.Session()
    # Provide a custom User‑Agent to comply with API etiquette.
    session.headers.update({
        "User-Agent": (
            "WikipediaAltChecker/1.0 (https://openai.com; contact via user)"
        )
    })
    params = {
        "action": "query",
        "list": "categorymembers",
        "cmtitle": category,
        "cmlimit": 500,
        "format": "json",
        "cmnamespace": 0,  # restrict to main namespace (articles)
    }
    members: List[str] = []
    while True:
        resp = session.get(base_url, params=params, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        cm = data.get("query", {}).get("categorymembers", [])
        for item in cm:
            title = item.get("title")
            if title is not None:
                members.append(title)
                if limit is not None and len(members) >= limit:
                    return members
        # Break if there are no more pages to continue.
        cont = data.get("continue")
        if not cont:
            break
        # Copy continuation parameters (e.g., cmcontinue) for next request.
        params.update(cont)
    return members


def fetch_page_wikitext(title: str, lang: str, session: requests.Session | None = None) -> str:
    """Fetch the raw wikitext of a page.

    The MediaWiki API stores content in slots (since 2018).  For most
    pages, the main slot contains the article wikitext.  This function
    queries the ``revisions`` endpoint to retrieve the text of the most
    recent revision.

    Args:
        title:  The title of the page whose wikitext is desired.
        lang:   Language code for the target Wikipedia.
        session: Optional requests.Session instance to reuse HTTP
            connections and headers.

    Returns:
        The wikitext string.  If the page has no content, returns an
        empty string.
    """
    if session is None:
        session = requests.Session()
        session.headers.update({
            "User-Agent": (
                "WikipediaAltChecker/1.0 (https://openai.com; contact via user)"
            )
        })
    url = f"https://{lang}.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "prop": "revisions",
        "rvprop": "content",
        "rvslots": "main",
        "format": "json",
        "titles": title,
    }
    resp = session.get(url, params=params, timeout=30)
    resp.raise_for_status()
    data = resp.json()
    pages = data.get("query", {}).get("pages", {})
    # Extract the first page dictionary (key is pageid)
    if not pages:
        return ""
    _, page = next(iter(pages.items()))
    revisions = page.get("revisions")
    if not revisions:
        return ""
    # Each revision has a dict of slots; main slot contains content.
    slots = revisions[0].get("slots", {})
    main_slot = slots.get("main", {})
    content = main_slot.get("*") or main_slot.get("content")
    return content or ""


def extract_image_links(wikitext: str) -> List[str]:
    """Return a list of image link wikitext from a page's wikitext.

    MediaWiki allows images to be inserted with several prefixes: ``File:``,
    ``Image:``, ``Ficheiro:`` and ``Imagem:`` are common.  This function
    uses a regular expression to capture everything from ``[[`` through
    ``]]`` for these prefixes, including any optional parameters and
    trailing caption text.

    Args:
        wikitext: The raw wikitext of the page.

    Returns:
        A list of matched image link strings.
    """
    # Construct a regex pattern to match image/file links.  We match
    # everything non‑greedy up to the closing brackets.  The prefix
    # alternation is case‑insensitive.
    pattern = re.compile(
        r"\[\[\s*(?:File|Image|Ficheiro|Imagem)\s*:[^\]]+?\]\]",
        re.IGNORECASE,
    )
    return pattern.findall(wikitext)


def classify_alt(image_links: Iterable[str]) -> Tuple[List[str], List[str]]:
    """Partition image links into those with and without an alt parameter.

    Image links in wikitext may include many pipe‑separated parameters,
    such as ``thumb``, ``upright`` or ``alt=...``.  This function
    determines whether an image link contains an ``alt`` parameter (e.g.
    ``|alt=description``) or an indexed variant like ``alt1=``.  Links
    lacking any alt parameter are returned separately.

    Args:
        image_links: An iterable of raw image link strings.

    Returns:
        A tuple ``(with_alt, without_alt)`` where each element is a list
        of image link strings.
    """
    with_alt: List[str] = []
    without_alt: List[str] = []
    # Regex to detect alt or altN parameter; the ``\d*`` allows for
    # optional numeric suffixes (alt1, alt2, etc.).
    alt_pattern = re.compile(r"\|\s*alt\d*\s*=", re.IGNORECASE)
    for link in image_links:
        if alt_pattern.search(link):
            with_alt.append(link)
        else:
            without_alt.append(link)
    return with_alt, without_alt


def analyse_category(category: str, lang: str, limit: int | None = None) -> None:
    """Process all pages in a category and report alt statistics.

    For each page in the specified category, this function fetches the
    wikitext, extracts image links, partitions them based on the
    presence of an ``alt`` parameter and prints a summary.  After
    processing all pages, it prints aggregated totals.

    Args:
        category: Category name supplied by the user (may lack prefix).
        lang:     Language code for the target Wikipedia.
        limit:    Optional maximum number of pages to process.
    """
    normalized = normalize_category_name(category, lang)
    titles = get_category_members(normalized, lang, limit=limit)
    if not titles:
        print(f"No pages found in {normalized} (lang={lang})")
        return
    session = requests.Session()
    session.headers.update({
        "User-Agent": (
            "WikipediaAltChecker/1.0 (https://openai.com; contact via user)"
        )
    })
    total_with = 0
    total_without = 0
    total_images = 0
    print(f"Analysing {len(titles)} pages from category {normalized}\n")
    for title in titles:
        wikitext = fetch_page_wikitext(title, lang, session=session)
        if not wikitext:
            print(f"* {title}: no content")
            continue
        images = extract_image_links(wikitext)
        if not images:
            print(f"* {title}: no images")
            continue
        with_alt, without_alt = classify_alt(images)
        count_with = len(with_alt)
        count_without = len(without_alt)
        count_total = len(images)
        total_with += count_with
        total_without += count_without
        total_images += count_total
        print(
            f"* {title}: {count_total} image(s), {count_with} with alt, {count_without} without alt"
        )
    # Print aggregated summary
    print("\nSummary:")
    print(f"Total images analysed: {total_images}")
    print(f"With alt text:       {total_with}")
    print(f"Without alt text:    {total_without}")


def main(argv: List[str] | None = None) -> int:
    """Entry point when run as a script."""
    parser = argparse.ArgumentParser(
        description=(
            "Inspect a Wikipedia category and check images for alt text.\n\n"
            "Example: python wikipedia_alt_checker.py --category 'Animais' --lang pt"
        ),
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--category",
        "-c",
        required=True,
        help="Name of the category to process. Add quotes if it contains spaces.",
    )
    parser.add_argument(
        "--lang",
        "-l",
        default="pt",
        help="Language code for the Wikipedia edition (default: pt)",
    )
    parser.add_argument(
        "--limit",
        "-n",
        type=int,
        default=None,
        help="Optional maximum number of pages to inspect.",
    )
    args = parser.parse_args(argv) # Pass argv to parse_args
    try:
        analyse_category(args.category, args.lang, args.limit)
    except requests.HTTPError as http_err:
        print(f"HTTP error: {http_err}", file=sys.stderr)
        return 1
    except requests.RequestException as req_err:
        print(f"Request failed: {req_err}", file=sys.stderr)
        return 1
    except Exception as err:
        print(f"Error: {err}", file=sys.stderr)
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover -- CLI entry point
    # Use a placeholder list for argv when running directly in the notebook
    # You can replace this with actual arguments when running as a script
    sys.exit(main(['--category', 'Audiologia', '--lang', 'pt']))